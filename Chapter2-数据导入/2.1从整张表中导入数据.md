<h2>导入数据</h2>
在接下来的几节中，致力于从关系型数据，数据仓库中传输数据到Hadoop系统中。在本节中将演示和简介在不同的情况下将如何将关系型数据中一张表导入到Hadoop中。

接下来将会通过实例来讲解sqoop的不同的特征，而这些的实例可以直接拷贝到运行。为了能够实现这点，首先需要安装一个关系型数据库。本书使用的是Mysql数据库，这个数据库用户名和密码都是sqoop,连接使用的数据库名也为sqoop。
你可以使用mysql.credentials.sql脚本来进行创建这个数据库，这个脚本可以从和本书关联的Github工程中下载。

你可以改变本示例，作用于不同的数据库系统。进一步的细节在本书后面说明。由于sqoop主要的关注在数据的传输，
，所以在进行示例演示的时候，确保在关系型数据库中，已有数据，所以创建了一张cities表，包含少许的城市信息，可以使用 mysql.tables.sql脚本，这个脚本可以从上文已经提到的[Github project](https://github.com/jarcec/Apache-Sqoop-Cookbook)项目中下载。

![](/assets/chapter2_cities.png)

<h2>2.1传送整张表</h2>

<h3>问题</h3>
在关系型数据库中有一张表，需要将这张表的内容传输到Hadoop的HDFS文件系统上。

<h3>解决方案</h3>
通过Sqoop导入一张表非常简单:使用 **import** 操作 ，指定数据库和认证信息和需要导出表的名字，如下:

```
sqoop import \
    --connect jdbc:mysql://localhost:3306/sqoop \
    --username sqoop \
    --password sqoop \
    --table cities
```

![](/assets/chapter2_import.png)

<h3>讨论</h3>
导入整张表,是使用sqoop最普遍和直接的方式的之一了。执行了上述的命令后，将会生成一个对应的csv文件，
在这个csv文件的每一行对应数据库中的一行。

![](/assets/chapter2_importdatalike.png)

在上述示例中，sqoop调用过程中传递了一系列的参数，接下来将对这些参数细节进行说明.
在sqoop可执行程序后第一个参数是 **import** , import指出了合适的tool命令。对于import tool命令
被用于在你想从关系数据中将数据导入到Hadoop中的时候。稍后将讨论export导出命令,这个命令的作用是相反的。
接下来的一个参数 **--connect** 中包含了定位到数据库的JDBC URL地址。对于每个数据的URL都是特有的，
所以需要查阅数据库操作文档,以使用正确的url格式。在连接url后的两个参数 **--username** 和 **--password** , 这两个参数是sqoop在连接数据库时需要使用的凭据。
最后一个参数 **--table** 是传输数据的表名。

现在初步的明确了各个参数的作用了，接下来深入一点来看看在执行了import的命令后会发生什么。
* 首先 sqoop连接到数据库 获取表的元数据：表有多少列，列名是什么及其数据类型.如 对应 cities 表而言
  sqoop将会获取的信息是 : 3列 id country city 类型分别是 int , varchar , varchar ，对于特定
  的数据库系统和表本身的信息，其他有用的元数据信息同样可以获取到。在这个阶段，sqoop并没有在数据库和
  hadoop间有数据的传输；但是会有对目录表和视图的查询，基于获取的元数据信息。sqoop将会生成java类
  并通过JDK和Hadoop中的有效库进行编译。

* 其次 sqoop连接到Hadoop集群并且提交一个MapReduce作业，在作业中的每个mapper将会传输表中的部分数据
  由于MapReduce在同一时间可以执行多个mapper操作，sqoop将会利用数据库系统潜在性能，以并行的方式传输数据
  已达到更好的性能。对于每个mapper中直接在数据库表和hadoop集群间传输数据。为了避免数据传输中瓶颈，处于
  中间位置的sqoop扮演着监控者角色，而不是数据传输的参与者，这也是Sqoop设计的一个重要原则。
  

